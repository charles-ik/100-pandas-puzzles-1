{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 pandas puzzles\n",
    "\n",
    "Inspired by [100 Numpy exerises](https://github.com/rougier/numpy-100), here are 100* short puzzles for testing your knowledge of [pandas'](http://pandas.pydata.org/) power.\n",
    "\n",
    "Since pandas is a large library with many different specialist features and functions, these excercises focus mainly on the fundamentals of manipulating data (indexing, grouping, aggregating, cleaning), making use of the core DataFrame and Series objects. \n",
    "\n",
    "Many of the excerises here are stright-forward in that the solutions require no more than a few lines of code (in pandas or NumPy... don't go using pure Python or Cython!). Choosing the right methods and following best practices is the underlying goal.\n",
    "\n",
    "The exercises are loosely divided in sections. Each section has a difficulty rating; these ratings are subjective, of course, but should be a seen as a rough guide as to how inventive the required solution is.\n",
    "\n",
    "If you're just starting out with pandas and you are looking for some other resources, the official documentation  is very extensive. In particular, some good places get a broader overview of pandas are...\n",
    "\n",
    "- [10 minutes to pandas](http://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "- [pandas basics](http://pandas.pydata.org/pandas-docs/stable/basics.html)\n",
    "- [tutorials](http://pandas.pydata.org/pandas-docs/stable/tutorials.html)\n",
    "- [cookbook and idioms](http://pandas.pydata.org/pandas-docs/stable/cookbook.html#cookbook)\n",
    "\n",
    "Enjoy the puzzles!\n",
    "\n",
    "\\* *the list of exercises is not yet complete! Pull requests or suggestions for additional exercises, corrections and improvements are welcomed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing pandas\n",
    "\n",
    "### Getting started and checking your pandas setup\n",
    "\n",
    "Difficulty: *easy* \n",
    "\n",
    "**1.** Import pandas under the alias `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas>=0.25.0 (from -r requirements.txt (line 1))\n",
      "  Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting matplotlib>=2.1.1 (from -r requirements.txt (line 2))\n",
      "  Downloading matplotlib-3.10.0-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting numpy>=1.17.0 (from -r requirements.txt (line 3))\n",
      "  Downloading numpy-2.2.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting jupyter (from -r requirements.txt (line 4))\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=0.25.0->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=0.25.0->-r requirements.txt (line 1))\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=0.25.0->-r requirements.txt (line 1))\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=2.1.1->-r requirements.txt (line 2))\n",
      "  Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=2.1.1->-r requirements.txt (line 2))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=2.1.1->-r requirements.txt (line 2))\n",
      "  Downloading fonttools-4.56.0-cp313-cp313-win_amd64.whl.metadata (103 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=2.1.1->-r requirements.txt (line 2))\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib>=2.1.1->-r requirements.txt (line 2)) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib>=2.1.1->-r requirements.txt (line 2))\n",
      "  Downloading pillow-11.1.0-cp313-cp313-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=2.1.1->-r requirements.txt (line 2))\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting notebook (from jupyter->-r requirements.txt (line 4))\n",
      "  Downloading notebook-7.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter-console (from jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter->-r requirements.txt (line 4))\n",
      "  Downloading nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from jupyter->-r requirements.txt (line 4)) (6.29.5)\n",
      "Collecting ipywidgets (from jupyter->-r requirements.txt (line 4))\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jupyterlab (from jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25.0->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (1.8.12)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (8.32.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (6.1.1)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 4)) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-console->jupyter->-r requirements.txt (line 4)) (3.0.50)\n",
      "Requirement already satisfied: pygments in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-console->jupyter->-r requirements.txt (line 4)) (2.19.1)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx>=0.25.0 (from jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jinja2>=3.0.3 (from jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting setuptools>=40.8.0 (from jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting markupsafe>=2.0 (from nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading mistune-3.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting nbformat>=5.7 (from nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting anyio (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 4)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 4)) (0.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 4)) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r requirements.txt (line 4)) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r requirements.txt (line 4)) (308)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pywinpty>=2.0.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading pywinpty-2.0.15-cp313-cp313-win_amd64.whl.metadata (5.2 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting requests>=2.31 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.7->nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->-r requirements.txt (line 4)) (0.2.13)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 4)) (0.8.4)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading rpds_py-0.22.3-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pyyaml>=5.3 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\cikeg\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 4)) (0.2.3)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 4))\n",
      "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
      "Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 4.5/11.5 MB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 32.0 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.0-cp313-cp313-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 6.8/8.0 MB 32.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 29.7 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.2-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 37.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 37.7 MB/s eta 0:00:00\n",
      "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl (220 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 35.6 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.1.0-cp313-cp313-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 39.3 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
      "   ---------------------------------------- 0.0/11.7 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 9.4/11.7 MB 48.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.7/11.7 MB 44.8 MB/s eta 0:00:00\n",
      "Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Downloading notebook-7.3.2-py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 11.0/13.2 MB 52.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 48.4 MB/s eta 0:00:00\n",
      "Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "Downloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
      "Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Downloading mistune-3.1.1-py3-none-any.whl (53 kB)\n",
      "Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 34.0 MB/s eta 0:00:00\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 44.3 MB/s eta 0:00:00\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  10.0/10.2 MB 50.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 45.3 MB/s eta 0:00:00\n",
      "Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading prometheus_client-0.21.1-py3-none-any.whl (54 kB)\n",
      "Downloading pywinpty-2.0.15-cp313-cp313-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 36.7 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Downloading terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading rpds_py-0.22.3-cp313-cp313-win_amd64.whl (235 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
      "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl (182 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: webencodings, pytz, fastjsonschema, widgetsnbextension, websocket-client, webcolors, urllib3, uri-template, tzdata, typing-extensions, types-python-dateutil, tinycss2, soupsieve, sniffio, setuptools, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, pyyaml, pywinpty, python-json-logger, pyparsing, pycparser, prometheus-client, pillow, pandocfilters, overrides, numpy, mistune, markupsafe, kiwisolver, jupyterlab-widgets, jupyterlab-pygments, jsonpointer, json5, idna, h11, fqdn, fonttools, defusedxml, cycler, charset-normalizer, certifi, bleach, babel, attrs, async-lru, terminado, requests, referencing, pandas, jinja2, httpcore, contourpy, cffi, beautifulsoup4, arrow, anyio, matplotlib, jupyter-server-terminals, jsonschema-specifications, isoduration, ipywidgets, httpx, argon2-cffi-bindings, jupyter-console, jsonschema, argon2-cffi, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "Successfully installed anyio-4.8.0 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.4 attrs-25.1.0 babel-2.17.0 beautifulsoup4-4.13.3 bleach-6.2.0 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.1 contourpy-1.3.1 cycler-0.12.1 defusedxml-0.7.1 fastjsonschema-2.21.1 fonttools-4.56.0 fqdn-1.5.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 ipywidgets-8.1.5 isoduration-20.11.0 jinja2-3.1.5 json5-0.10.0 jsonpointer-3.0.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.5 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab-widgets-3.0.13 kiwisolver-1.4.8 markupsafe-3.0.2 matplotlib-3.10.0 mistune-3.1.1 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 notebook-7.3.2 notebook-shim-0.2.4 numpy-2.2.2 overrides-7.7.0 pandas-2.2.3 pandocfilters-1.5.1 pillow-11.1.0 prometheus-client-0.21.1 pycparser-2.22 pyparsing-3.2.1 python-json-logger-3.2.1 pytz-2025.1 pywinpty-2.0.15 pyyaml-6.0.2 referencing-0.36.2 requests-2.32.3 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.22.3 send2trash-1.8.3 setuptools-75.8.0 sniffio-1.3.1 soupsieve-2.6 terminado-0.18.1 tinycss2-1.4.0 types-python-dateutil-2.9.0.20241206 typing-extensions-4.12.2 tzdata-2025.1 uri-template-1.3.0 urllib3-2.3.0 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 widgetsnbextension-4.0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Print the version of pandas that has been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Print out all the *version* information of the libraries that are required by the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': 'pandas',\n",
       " '__doc__': '\\npandas - a powerful data analysis and manipulation library for Python\\n=====================================================================\\n\\n**pandas** is a Python package providing fast, flexible, and expressive data\\nstructures designed to make working with \"relational\" or \"labeled\" data both\\neasy and intuitive. It aims to be the fundamental high-level building block for\\ndoing practical, **real world** data analysis in Python. Additionally, it has\\nthe broader goal of becoming **the most powerful and flexible open source data\\nanalysis / manipulation tool available in any language**. It is already well on\\nits way toward this goal.\\n\\nMain Features\\n-------------\\nHere are just a few of the things that pandas does well:\\n\\n  - Easy handling of missing data in floating point as well as non-floating\\n    point data.\\n  - Size mutability: columns can be inserted and deleted from DataFrame and\\n    higher dimensional objects\\n  - Automatic and explicit data alignment: objects can be explicitly aligned\\n    to a set of labels, or the user can simply ignore the labels and let\\n    `Series`, `DataFrame`, etc. automatically align the data for you in\\n    computations.\\n  - Powerful, flexible group by functionality to perform split-apply-combine\\n    operations on data sets, for both aggregating and transforming data.\\n  - Make it easy to convert ragged, differently-indexed data in other Python\\n    and NumPy data structures into DataFrame objects.\\n  - Intelligent label-based slicing, fancy indexing, and subsetting of large\\n    data sets.\\n  - Intuitive merging and joining data sets.\\n  - Flexible reshaping and pivoting of data sets.\\n  - Hierarchical labeling of axes (possible to have multiple labels per tick).\\n  - Robust IO tools for loading data from flat files (CSV and delimited),\\n    Excel files, databases, and saving/loading data from the ultrafast HDF5\\n    format.\\n  - Time series-specific functionality: date range generation and frequency\\n    conversion, moving window statistics, date shifting and lagging.\\n',\n",
       " '__package__': 'pandas',\n",
       " '__loader__': <_frozen_importlib_external.SourceFileLoader at 0x23f2e9973b0>,\n",
       " '__spec__': ModuleSpec(name='pandas', loader=<_frozen_importlib_external.SourceFileLoader object at 0x0000023F2E9973B0>, origin='c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\__init__.py', submodule_search_locations=['c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas']),\n",
       " '__path__': ['c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas'],\n",
       " '__file__': 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\__init__.py',\n",
       " '__cached__': 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\__pycache__\\\\__init__.cpython-313.pyc',\n",
       " '__builtins__': {'__name__': 'builtins',\n",
       "  '__doc__': \"Built-in functions, types, exceptions, and other objects.\\n\\nThis module provides direct access to all 'built-in'\\nidentifiers of Python; for example, builtins.len is\\nthe full name for the built-in function len().\\n\\nThis module is not normally accessed explicitly by most\\napplications, but can be useful in modules that provide\\nobjects with the same name as a built-in value, but in\\nwhich the built-in of that name is also needed.\",\n",
       "  '__package__': '',\n",
       "  '__loader__': _frozen_importlib.BuiltinImporter,\n",
       "  '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'),\n",
       "  '__build_class__': <function __build_class__>,\n",
       "  '__import__': <function __import__(name, globals=None, locals=None, fromlist=(), level=0)>,\n",
       "  'abs': <function abs(x, /)>,\n",
       "  'all': <function all(iterable, /)>,\n",
       "  'any': <function any(iterable, /)>,\n",
       "  'ascii': <function ascii(obj, /)>,\n",
       "  'bin': <function bin(number, /)>,\n",
       "  'breakpoint': <function breakpoint(*args, **kws)>,\n",
       "  'callable': <function callable(obj, /)>,\n",
       "  'chr': <function chr(i, /)>,\n",
       "  'compile': <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       "  'delattr': <function delattr(obj, name, /)>,\n",
       "  'dir': <function dir>,\n",
       "  'divmod': <function divmod(x, y, /)>,\n",
       "  'eval': <function eval(source, /, globals=None, locals=None)>,\n",
       "  'exec': <function exec(source, /, globals=None, locals=None, *, closure=None)>,\n",
       "  'format': <function format(value, format_spec='', /)>,\n",
       "  'getattr': <function getattr>,\n",
       "  'globals': <function globals()>,\n",
       "  'hasattr': <function hasattr(obj, name, /)>,\n",
       "  'hash': <function hash(obj, /)>,\n",
       "  'hex': <function hex(number, /)>,\n",
       "  'id': <function id(obj, /)>,\n",
       "  'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x0000023F2D8D9E80>>,\n",
       "  'isinstance': <function isinstance(obj, class_or_tuple, /)>,\n",
       "  'issubclass': <function issubclass(cls, class_or_tuple, /)>,\n",
       "  'iter': <function iter>,\n",
       "  'aiter': <function aiter(async_iterable, /)>,\n",
       "  'len': <function len(obj, /)>,\n",
       "  'locals': <function locals()>,\n",
       "  'max': <function max>,\n",
       "  'min': <function min>,\n",
       "  'next': <function next>,\n",
       "  'anext': <function anext>,\n",
       "  'oct': <function oct(number, /)>,\n",
       "  'ord': <function ord(c, /)>,\n",
       "  'pow': <function pow(base, exp, mod=None)>,\n",
       "  'print': <function print(*args, sep=' ', end='\\n', file=None, flush=False)>,\n",
       "  'repr': <function repr(obj, /)>,\n",
       "  'round': <function round(number, ndigits=None)>,\n",
       "  'setattr': <function setattr(obj, name, value, /)>,\n",
       "  'sorted': <function sorted(iterable, /, *, key=None, reverse=False)>,\n",
       "  'sum': <function sum(iterable, /, start=0)>,\n",
       "  'vars': <function vars>,\n",
       "  'None': None,\n",
       "  'Ellipsis': Ellipsis,\n",
       "  'NotImplemented': NotImplemented,\n",
       "  'False': False,\n",
       "  'True': True,\n",
       "  'bool': bool,\n",
       "  'memoryview': memoryview,\n",
       "  'bytearray': bytearray,\n",
       "  'bytes': bytes,\n",
       "  'classmethod': classmethod,\n",
       "  'complex': complex,\n",
       "  'dict': dict,\n",
       "  'enumerate': enumerate,\n",
       "  'filter': filter,\n",
       "  'float': float,\n",
       "  'frozenset': frozenset,\n",
       "  'property': property,\n",
       "  'int': int,\n",
       "  'list': list,\n",
       "  'map': map,\n",
       "  'object': object,\n",
       "  'range': range,\n",
       "  'reversed': reversed,\n",
       "  'set': set,\n",
       "  'slice': slice,\n",
       "  'staticmethod': staticmethod,\n",
       "  'str': str,\n",
       "  'super': super,\n",
       "  'tuple': tuple,\n",
       "  'type': type,\n",
       "  'zip': zip,\n",
       "  '__debug__': True,\n",
       "  'BaseException': BaseException,\n",
       "  'BaseExceptionGroup': BaseExceptionGroup,\n",
       "  'Exception': Exception,\n",
       "  'GeneratorExit': GeneratorExit,\n",
       "  'KeyboardInterrupt': KeyboardInterrupt,\n",
       "  'SystemExit': SystemExit,\n",
       "  'ArithmeticError': ArithmeticError,\n",
       "  'AssertionError': AssertionError,\n",
       "  'AttributeError': AttributeError,\n",
       "  'BufferError': BufferError,\n",
       "  'EOFError': EOFError,\n",
       "  'ImportError': ImportError,\n",
       "  'LookupError': LookupError,\n",
       "  'MemoryError': MemoryError,\n",
       "  'NameError': NameError,\n",
       "  'OSError': OSError,\n",
       "  'ReferenceError': ReferenceError,\n",
       "  'RuntimeError': RuntimeError,\n",
       "  'StopAsyncIteration': StopAsyncIteration,\n",
       "  'StopIteration': StopIteration,\n",
       "  'SyntaxError': SyntaxError,\n",
       "  'SystemError': SystemError,\n",
       "  'TypeError': TypeError,\n",
       "  'ValueError': ValueError,\n",
       "  'Warning': Warning,\n",
       "  'FloatingPointError': FloatingPointError,\n",
       "  'OverflowError': OverflowError,\n",
       "  'ZeroDivisionError': ZeroDivisionError,\n",
       "  'BytesWarning': BytesWarning,\n",
       "  'DeprecationWarning': DeprecationWarning,\n",
       "  'EncodingWarning': EncodingWarning,\n",
       "  'FutureWarning': FutureWarning,\n",
       "  'ImportWarning': ImportWarning,\n",
       "  'PendingDeprecationWarning': PendingDeprecationWarning,\n",
       "  'ResourceWarning': ResourceWarning,\n",
       "  'RuntimeWarning': RuntimeWarning,\n",
       "  'SyntaxWarning': SyntaxWarning,\n",
       "  'UnicodeWarning': UnicodeWarning,\n",
       "  'UserWarning': UserWarning,\n",
       "  'BlockingIOError': BlockingIOError,\n",
       "  'ChildProcessError': ChildProcessError,\n",
       "  'ConnectionError': ConnectionError,\n",
       "  'FileExistsError': FileExistsError,\n",
       "  'FileNotFoundError': FileNotFoundError,\n",
       "  'InterruptedError': InterruptedError,\n",
       "  'IsADirectoryError': IsADirectoryError,\n",
       "  'NotADirectoryError': NotADirectoryError,\n",
       "  'PermissionError': PermissionError,\n",
       "  'ProcessLookupError': ProcessLookupError,\n",
       "  'TimeoutError': TimeoutError,\n",
       "  'IndentationError': IndentationError,\n",
       "  '_IncompleteInputError': _IncompleteInputError,\n",
       "  'IndexError': IndexError,\n",
       "  'KeyError': KeyError,\n",
       "  'ModuleNotFoundError': ModuleNotFoundError,\n",
       "  'NotImplementedError': NotImplementedError,\n",
       "  'PythonFinalizationError': PythonFinalizationError,\n",
       "  'RecursionError': RecursionError,\n",
       "  'UnboundLocalError': UnboundLocalError,\n",
       "  'UnicodeError': UnicodeError,\n",
       "  'BrokenPipeError': BrokenPipeError,\n",
       "  'ConnectionAbortedError': ConnectionAbortedError,\n",
       "  'ConnectionRefusedError': ConnectionRefusedError,\n",
       "  'ConnectionResetError': ConnectionResetError,\n",
       "  'TabError': TabError,\n",
       "  'UnicodeDecodeError': UnicodeDecodeError,\n",
       "  'UnicodeEncodeError': UnicodeEncodeError,\n",
       "  'UnicodeTranslateError': UnicodeTranslateError,\n",
       "  'ExceptionGroup': ExceptionGroup,\n",
       "  'EnvironmentError': OSError,\n",
       "  'IOError': OSError,\n",
       "  'WindowsError': OSError,\n",
       "  'open': <function _io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       "  'copyright': Copyright (c) 2001-2024 Python Software Foundation.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 2000 BeOpen.com.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
       "  All Rights Reserved.\n",
       "  \n",
       "  Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
       "  All Rights Reserved.,\n",
       "  'credits':     Thanks to CWI, CNRI, BeOpen, Zope Corporation, the Python Software\n",
       "      Foundation, and a cast of thousands for supporting Python\n",
       "      development.  See www.python.org for more information.,\n",
       "  'license': Type license() to see the full license text,\n",
       "  'help': Type help() for interactive help, or help(object) for help about object.,\n",
       "  'execfile': <function _pydev_bundle._pydev_execfile.execfile(file, glob=None, loc=None)>,\n",
       "  'runfile': <function _pydev_bundle.pydev_umd.runfile(filename, args=None, wdir=None, namespace=None)>,\n",
       "  '__IPYTHON__': True,\n",
       "  'display': <function IPython.core.display_functions.display(*objs, include=None, exclude=None, metadata=None, transient=None, display_id=None, raw=False, clear=False, **kwargs)>,\n",
       "  'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000023F2E1A7E00>>},\n",
       " 'annotations': _Feature((3, 7, 0, 'beta', 1), None, 16777216),\n",
       " '__docformat__': 'restructuredtext',\n",
       " 'util': <module 'pandas.util' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\util\\\\__init__.py'>,\n",
       " 'compat': <module 'pandas.compat' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\compat\\\\__init__.py'>,\n",
       " '_is_numpy_dev': False,\n",
       " '_typing': <module 'pandas._typing' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\_typing.py'>,\n",
       " '_config': <module 'pandas._config' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\_config\\\\__init__.py'>,\n",
       " 'get_option': <pandas._config.config.CallableDynamicDoc at 0x23f44b5b770>,\n",
       " 'set_option': <pandas._config.config.CallableDynamicDoc at 0x23f44bab390>,\n",
       " 'reset_option': <pandas._config.config.CallableDynamicDoc at 0x23f44c94410>,\n",
       " 'describe_option': <pandas._config.config.CallableDynamicDoc at 0x23f44c0df30>,\n",
       " 'option_context': pandas._config.config.option_context,\n",
       " 'options': <pandas._config.config.DictWrapper at 0x23f44b5b8c0>,\n",
       " 'core': <module 'pandas.core' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\core\\\\__init__.py'>,\n",
       " 'pandas': <module 'pandas' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\__init__.py'>,\n",
       " '_pandas_parser_CAPI': <capsule object \"pandas._pandas_parser_CAPI\" at 0x0000023F44D10400>,\n",
       " '_pandas_datetime_CAPI': <capsule object \"pandas._pandas_datetime_CAPI\" at 0x0000023F44D10450>,\n",
       " '_libs': <module 'pandas._libs' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\_libs\\\\__init__.py'>,\n",
       " 'errors': <module 'pandas.errors' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\errors\\\\__init__.py'>,\n",
       " 'io': <module 'pandas.io' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\__init__.py'>,\n",
       " 'tseries': <module 'pandas.tseries' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\tseries\\\\__init__.py'>,\n",
       " 'arrays': <module 'pandas.arrays' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\arrays\\\\__init__.py'>,\n",
       " 'plotting': <module 'pandas.plotting' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\plotting\\\\__init__.py'>,\n",
       " 'ArrowDtype': pandas.core.dtypes.dtypes.ArrowDtype,\n",
       " 'Int8Dtype': pandas.core.arrays.integer.Int8Dtype,\n",
       " 'Int16Dtype': pandas.core.arrays.integer.Int16Dtype,\n",
       " 'Int32Dtype': pandas.core.arrays.integer.Int32Dtype,\n",
       " 'Int64Dtype': pandas.core.arrays.integer.Int64Dtype,\n",
       " 'UInt8Dtype': pandas.core.arrays.integer.UInt8Dtype,\n",
       " 'UInt16Dtype': pandas.core.arrays.integer.UInt16Dtype,\n",
       " 'UInt32Dtype': pandas.core.arrays.integer.UInt32Dtype,\n",
       " 'UInt64Dtype': pandas.core.arrays.integer.UInt64Dtype,\n",
       " 'Float32Dtype': pandas.core.arrays.floating.Float32Dtype,\n",
       " 'Float64Dtype': pandas.core.arrays.floating.Float64Dtype,\n",
       " 'CategoricalDtype': pandas.core.dtypes.dtypes.CategoricalDtype,\n",
       " 'PeriodDtype': pandas.core.dtypes.dtypes.PeriodDtype,\n",
       " 'IntervalDtype': pandas.core.dtypes.dtypes.IntervalDtype,\n",
       " 'DatetimeTZDtype': pandas.core.dtypes.dtypes.DatetimeTZDtype,\n",
       " 'StringDtype': pandas.core.arrays.string_.StringDtype,\n",
       " 'BooleanDtype': pandas.core.arrays.boolean.BooleanDtype,\n",
       " 'NA': <NA>,\n",
       " 'isna': <function pandas.core.dtypes.missing.isna(obj: 'object') -> 'bool | npt.NDArray[np.bool_] | NDFrame'>,\n",
       " 'isnull': <function pandas.core.dtypes.missing.isna(obj: 'object') -> 'bool | npt.NDArray[np.bool_] | NDFrame'>,\n",
       " 'notna': <function pandas.core.dtypes.missing.notna(obj: 'object') -> 'bool | npt.NDArray[np.bool_] | NDFrame'>,\n",
       " 'notnull': <function pandas.core.dtypes.missing.notna(obj: 'object') -> 'bool | npt.NDArray[np.bool_] | NDFrame'>,\n",
       " 'Index': pandas.core.indexes.base.Index,\n",
       " 'CategoricalIndex': pandas.core.indexes.category.CategoricalIndex,\n",
       " 'RangeIndex': pandas.core.indexes.range.RangeIndex,\n",
       " 'MultiIndex': pandas.core.indexes.multi.MultiIndex,\n",
       " 'IntervalIndex': pandas.core.indexes.interval.IntervalIndex,\n",
       " 'TimedeltaIndex': pandas.core.indexes.timedeltas.TimedeltaIndex,\n",
       " 'DatetimeIndex': pandas.core.indexes.datetimes.DatetimeIndex,\n",
       " 'PeriodIndex': pandas.core.indexes.period.PeriodIndex,\n",
       " 'IndexSlice': <pandas.core.indexing._IndexSlice at 0x23f4523fcb0>,\n",
       " 'NaT': NaT,\n",
       " 'Period': pandas._libs.tslibs.period.Period,\n",
       " 'period_range': <function pandas.core.indexes.period.period_range(start=None, end=None, periods: 'int | None' = None, freq=None, name: 'Hashable | None' = None) -> 'PeriodIndex'>,\n",
       " 'Timedelta': pandas._libs.tslibs.timedeltas.Timedelta,\n",
       " 'timedelta_range': <function pandas.core.indexes.timedeltas.timedelta_range(start=None, end=None, periods: 'int | None' = None, freq=None, name=None, closed=None, *, unit: 'str | None' = None) -> 'TimedeltaIndex'>,\n",
       " 'Timestamp': pandas._libs.tslibs.timestamps.Timestamp,\n",
       " 'date_range': <function pandas.core.indexes.datetimes.date_range(start=None, end=None, periods=None, freq=None, tz=None, normalize: 'bool' = False, name: 'Hashable | None' = None, inclusive: 'IntervalClosedType' = 'both', *, unit: 'str | None' = None, **kwargs) -> 'DatetimeIndex'>,\n",
       " 'bdate_range': <function pandas.core.indexes.datetimes.bdate_range(start=None, end=None, periods: 'int | None' = None, freq: 'Frequency | dt.timedelta' = 'B', tz=None, normalize: 'bool' = True, name: 'Hashable | None' = None, weekmask=None, holidays=None, inclusive: 'IntervalClosedType' = 'both', **kwargs) -> 'DatetimeIndex'>,\n",
       " 'Interval': pandas._libs.interval.Interval,\n",
       " 'interval_range': <function pandas.core.indexes.interval.interval_range(start=None, end=None, periods=None, freq=None, name: 'Hashable | None' = None, closed: 'IntervalClosedType' = 'right') -> 'IntervalIndex'>,\n",
       " 'DateOffset': pandas._libs.tslibs.offsets.DateOffset,\n",
       " 'to_numeric': <function pandas.core.tools.numeric.to_numeric(arg, errors: 'DateTimeErrorChoices' = 'raise', downcast: \"Literal['integer', 'signed', 'unsigned', 'float'] | None\" = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>)>,\n",
       " 'to_datetime': <function pandas.core.tools.datetimes.to_datetime(arg: 'DatetimeScalarOrArrayConvertible | DictConvertible', errors: 'DateTimeErrorChoices' = 'raise', dayfirst: 'bool' = False, yearfirst: 'bool' = False, utc: 'bool' = False, format: 'str | None' = None, exact: 'bool | lib.NoDefault' = <no_default>, unit: 'str | None' = None, infer_datetime_format: 'lib.NoDefault | bool' = <no_default>, origin: 'str' = 'unix', cache: 'bool' = True) -> 'DatetimeIndex | Series | DatetimeScalar | NaTType | None'>,\n",
       " 'to_timedelta': <function pandas.core.tools.timedeltas.to_timedelta(arg: 'str | int | float | timedelta | list | tuple | range | ArrayLike | Index | Series', unit: 'UnitChoices | None' = None, errors: 'DateTimeErrorChoices' = 'raise') -> 'Timedelta | TimedeltaIndex | Series'>,\n",
       " 'Flags': pandas.core.flags.Flags,\n",
       " 'Grouper': pandas.core.groupby.grouper.Grouper,\n",
       " 'factorize': <function pandas.core.algorithms.factorize(values, sort: 'bool' = False, use_na_sentinel: 'bool' = True, size_hint: 'int | None' = None) -> 'tuple[np.ndarray, np.ndarray | Index]'>,\n",
       " 'unique': <function pandas.core.algorithms.unique(values)>,\n",
       " 'value_counts': <function pandas.core.algorithms.value_counts(values, sort: 'bool' = True, ascending: 'bool' = False, normalize: 'bool' = False, bins=None, dropna: 'bool' = True) -> 'Series'>,\n",
       " 'NamedAgg': pandas.core.groupby.generic.NamedAgg,\n",
       " 'array': <function pandas.core.construction.array(data: 'Sequence[object] | AnyArrayLike', dtype: 'Dtype | None' = None, copy: 'bool' = True) -> 'ExtensionArray'>,\n",
       " 'Categorical': pandas.core.arrays.categorical.Categorical,\n",
       " 'set_eng_float_format': <function pandas.io.formats.format.set_eng_float_format(accuracy: 'int' = 3, use_eng_prefix: 'bool' = False) -> 'None'>,\n",
       " 'Series': pandas.core.series.Series,\n",
       " 'DataFrame': pandas.core.frame.DataFrame,\n",
       " 'SparseDtype': pandas.core.dtypes.dtypes.SparseDtype,\n",
       " 'infer_freq': <function pandas.tseries.frequencies.infer_freq(index: 'DatetimeIndex | TimedeltaIndex | Series | DatetimeLikeArrayMixin') -> 'str | None'>,\n",
       " 'offsets': <module 'pandas.tseries.offsets' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\tseries\\\\offsets.py'>,\n",
       " 'eval': <function pandas.core.computation.eval.eval(expr: 'str | BinOp', parser: 'str' = 'pandas', engine: 'str | None' = None, local_dict=None, global_dict=None, resolvers=(), level: 'int' = 0, target=None, inplace: 'bool' = False)>,\n",
       " 'concat': <function pandas.core.reshape.concat.concat(objs: 'Iterable[Series | DataFrame] | Mapping[HashableT, Series | DataFrame]', *, axis: 'Axis' = 0, join: 'str' = 'outer', ignore_index: 'bool' = False, keys: 'Iterable[Hashable] | None' = None, levels=None, names: 'list[HashableT] | None' = None, verify_integrity: 'bool' = False, sort: 'bool' = False, copy: 'bool | None' = None) -> 'DataFrame | Series'>,\n",
       " 'lreshape': <function pandas.core.reshape.melt.lreshape(data: 'DataFrame', groups: 'dict', dropna: 'bool' = True) -> 'DataFrame'>,\n",
       " 'melt': <function pandas.core.reshape.melt.melt(frame: 'DataFrame', id_vars=None, value_vars=None, var_name=None, value_name: 'Hashable' = 'value', col_level=None, ignore_index: 'bool' = True) -> 'DataFrame'>,\n",
       " 'wide_to_long': <function pandas.core.reshape.melt.wide_to_long(df: 'DataFrame', stubnames, i, j, sep: 'str' = '', suffix: 'str' = '\\\\d+') -> 'DataFrame'>,\n",
       " 'merge': <function pandas.core.reshape.merge.merge(left: 'DataFrame | Series', right: 'DataFrame | Series', how: 'MergeHow' = 'inner', on: 'IndexLabel | AnyArrayLike | None' = None, left_on: 'IndexLabel | AnyArrayLike | None' = None, right_on: 'IndexLabel | AnyArrayLike | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, sort: 'bool' = False, suffixes: 'Suffixes' = ('_x', '_y'), copy: 'bool | None' = None, indicator: 'str | bool' = False, validate: 'str | None' = None) -> 'DataFrame'>,\n",
       " 'merge_asof': <function pandas.core.reshape.merge.merge_asof(left: 'DataFrame | Series', right: 'DataFrame | Series', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_index: 'bool' = False, right_index: 'bool' = False, by=None, left_by=None, right_by=None, suffixes: 'Suffixes' = ('_x', '_y'), tolerance: 'int | Timedelta | None' = None, allow_exact_matches: 'bool' = True, direction: 'str' = 'backward') -> 'DataFrame'>,\n",
       " 'merge_ordered': <function pandas.core.reshape.merge.merge_ordered(left: 'DataFrame | Series', right: 'DataFrame | Series', on: 'IndexLabel | None' = None, left_on: 'IndexLabel | None' = None, right_on: 'IndexLabel | None' = None, left_by=None, right_by=None, fill_method: 'str | None' = None, suffixes: 'Suffixes' = ('_x', '_y'), how: 'JoinHow' = 'outer') -> 'DataFrame'>,\n",
       " 'crosstab': <function pandas.core.reshape.pivot.crosstab(index, columns, values=None, rownames=None, colnames=None, aggfunc=None, margins: 'bool' = False, margins_name: 'Hashable' = 'All', dropna: 'bool' = True, normalize: \"bool | Literal[0, 1, 'all', 'index', 'columns']\" = False) -> 'DataFrame'>,\n",
       " 'pivot': <function pandas.core.reshape.pivot.pivot(data: 'DataFrame', *, columns: 'IndexLabel', index: 'IndexLabel | lib.NoDefault' = <no_default>, values: 'IndexLabel | lib.NoDefault' = <no_default>) -> 'DataFrame'>,\n",
       " 'pivot_table': <function pandas.core.reshape.pivot.pivot_table(data: 'DataFrame', values=None, index=None, columns=None, aggfunc: 'AggFuncType' = 'mean', fill_value=None, margins: 'bool' = False, dropna: 'bool' = True, margins_name: 'Hashable' = 'All', observed: 'bool | lib.NoDefault' = <no_default>, sort: 'bool' = True) -> 'DataFrame'>,\n",
       " 'get_dummies': <function pandas.core.reshape.encoding.get_dummies(data, prefix=None, prefix_sep: 'str | Iterable[str] | dict[str, str]' = '_', dummy_na: 'bool' = False, columns=None, sparse: 'bool' = False, drop_first: 'bool' = False, dtype: 'NpDtype | None' = None) -> 'DataFrame'>,\n",
       " 'from_dummies': <function pandas.core.reshape.encoding.from_dummies(data: 'DataFrame', sep: 'None | str' = None, default_category: 'None | Hashable | dict[str, Hashable]' = None) -> 'DataFrame'>,\n",
       " 'cut': <function pandas.core.reshape.tile.cut(x, bins, right: 'bool' = True, labels=None, retbins: 'bool' = False, precision: 'int' = 3, include_lowest: 'bool' = False, duplicates: 'str' = 'raise', ordered: 'bool' = True)>,\n",
       " 'qcut': <function pandas.core.reshape.tile.qcut(x, q, labels=None, retbins: 'bool' = False, precision: 'int' = 3, duplicates: 'str' = 'raise')>,\n",
       " 'api': <module 'pandas.api' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\api\\\\__init__.py'>,\n",
       " '_testing': <module 'pandas._testing' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\_testing\\\\__init__.py'>,\n",
       " 'testing': <module 'pandas.testing' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\testing.py'>,\n",
       " 'show_versions': <function pandas.util._print_versions.show_versions(as_json: 'str | bool' = False) -> 'None'>,\n",
       " 'ExcelFile': pandas.io.excel._base.ExcelFile,\n",
       " 'ExcelWriter': pandas.io.excel._base.ExcelWriter,\n",
       " 'read_excel': <function pandas.io.excel._base.read_excel(io, sheet_name: 'str | int | list[IntStrT] | None' = 0, *, header: 'int | Sequence[int] | None' = 0, names: 'SequenceNotStr[Hashable] | range | None' = None, index_col: 'int | str | Sequence[int] | None' = None, usecols: 'int | str | Sequence[int] | Sequence[str] | Callable[[str], bool] | None' = None, dtype: 'DtypeArg | None' = None, engine: \"Literal['xlrd', 'openpyxl', 'odf', 'pyxlsb', 'calamine'] | None\" = None, converters: 'dict[str, Callable] | dict[int, Callable] | None' = None, true_values: 'Iterable[Hashable] | None' = None, false_values: 'Iterable[Hashable] | None' = None, skiprows: 'Sequence[int] | int | Callable[[int], object] | None' = None, nrows: 'int | None' = None, na_values=None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool' = False, parse_dates: 'list | dict | bool' = False, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'dict[Hashable, str] | str | None' = None, thousands: 'str | None' = None, decimal: 'str' = '.', comment: 'str | None' = None, skipfooter: 'int' = 0, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, engine_kwargs: 'dict | None' = None) -> 'DataFrame | dict[IntStrT, DataFrame]'>,\n",
       " 'read_csv': <function pandas.io.parsers.readers.read_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable] | None' = None, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: \"Literal['high', 'legacy'] | None\" = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'>,\n",
       " 'read_fwf': <function pandas.io.parsers.readers.read_fwf(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, colspecs: 'Sequence[tuple[int, int]] | str | None' = 'infer', widths: 'Sequence[int] | None' = None, infer_nrows: 'int' = 100, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, iterator: 'bool' = False, chunksize: 'int | None' = None, **kwds) -> 'DataFrame | TextFileReader'>,\n",
       " 'read_table': <function pandas.io.parsers.readers.read_table(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', *, sep: 'str | None | lib.NoDefault' = <no_default>, delimiter: 'str | None | lib.NoDefault' = None, header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer', names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>, index_col: 'IndexLabel | Literal[False] | None' = None, usecols: 'UsecolsArgType' = None, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters: 'Mapping[Hashable, Callable] | None' = None, true_values: 'list | None' = None, false_values: 'list | None' = None, skipinitialspace: 'bool' = False, skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None, skipfooter: 'int' = 0, nrows: 'int | None' = None, na_values: 'Sequence[str] | Mapping[str, Sequence[str]] | None' = None, keep_default_na: 'bool' = True, na_filter: 'bool' = True, verbose: 'bool | lib.NoDefault' = <no_default>, skip_blank_lines: 'bool' = True, parse_dates: 'bool | Sequence[Hashable]' = False, infer_datetime_format: 'bool | lib.NoDefault' = <no_default>, keep_date_col: 'bool | lib.NoDefault' = <no_default>, date_parser: 'Callable | lib.NoDefault' = <no_default>, date_format: 'str | dict[Hashable, str] | None' = None, dayfirst: 'bool' = False, cache_dates: 'bool' = True, iterator: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', thousands: 'str | None' = None, decimal: 'str' = '.', lineterminator: 'str | None' = None, quotechar: 'str' = '\"', quoting: 'int' = 0, doublequote: 'bool' = True, escapechar: 'str | None' = None, comment: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', dialect: 'str | csv.Dialect | None' = None, on_bad_lines: 'str' = 'error', delim_whitespace: 'bool | lib.NoDefault' = <no_default>, low_memory: 'bool' = True, memory_map: 'bool' = False, float_precision: 'str | None' = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | TextFileReader'>,\n",
       " 'read_pickle': <function pandas.io.pickle.read_pickle(filepath_or_buffer: 'FilePath | ReadPickleBuffer', compression: 'CompressionOptions' = 'infer', storage_options: 'StorageOptions | None' = None) -> 'DataFrame | Series'>,\n",
       " 'to_pickle': <function pandas.io.pickle.to_pickle(obj: 'Any', filepath_or_buffer: 'FilePath | WriteBuffer[bytes]', compression: 'CompressionOptions' = 'infer', protocol: 'int' = 5, storage_options: 'StorageOptions | None' = None) -> 'None'>,\n",
       " 'HDFStore': pandas.io.pytables.HDFStore,\n",
       " 'read_hdf': <function pandas.io.pytables.read_hdf(path_or_buf: 'FilePath | HDFStore', key=None, mode: 'str' = 'r', errors: 'str' = 'strict', where: 'str | list | None' = None, start: 'int | None' = None, stop: 'int | None' = None, columns: 'list[str] | None' = None, iterator: 'bool' = False, chunksize: 'int | None' = None, **kwargs)>,\n",
       " 'read_sql': <function pandas.io.sql.read_sql(sql, con, index_col: 'str | list[str] | None' = None, coerce_float: 'bool' = True, params=None, parse_dates=None, columns: 'list[str] | None' = None, chunksize: 'int | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, dtype: 'DtypeArg | None' = None) -> 'DataFrame | Iterator[DataFrame]'>,\n",
       " 'read_sql_query': <function pandas.io.sql.read_sql_query(sql, con, index_col: 'str | list[str] | None' = None, coerce_float: 'bool' = True, params: 'list[Any] | Mapping[str, Any] | None' = None, parse_dates: 'list[str] | dict[str, str] | None' = None, chunksize: 'int | None' = None, dtype: 'DtypeArg | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | Iterator[DataFrame]'>,\n",
       " 'read_sql_table': <function pandas.io.sql.read_sql_table(table_name: 'str', con, schema: 'str | None' = None, index_col: 'str | list[str] | None' = None, coerce_float: 'bool' = True, parse_dates: 'list[str] | dict[str, str] | None' = None, columns: 'list[str] | None' = None, chunksize: 'int | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame | Iterator[DataFrame]'>,\n",
       " 'read_clipboard': <function pandas.io.clipboards.read_clipboard(sep: 'str' = '\\\\s+', dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, **kwargs)>,\n",
       " 'read_parquet': <function pandas.io.parquet.read_parquet(path: 'FilePath | ReadBuffer[bytes]', engine: 'str' = 'auto', columns: 'list[str] | None' = None, storage_options: 'StorageOptions | None' = None, use_nullable_dtypes: 'bool | lib.NoDefault' = <no_default>, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, filesystem: 'Any' = None, filters: 'list[tuple] | list[list[tuple]] | None' = None, **kwargs) -> 'DataFrame'>,\n",
       " 'read_orc': <function pandas.io.orc.read_orc(path: 'FilePath | ReadBuffer[bytes]', columns: 'list[str] | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, filesystem: 'pyarrow.fs.FileSystem | fsspec.spec.AbstractFileSystem | None' = None, **kwargs: 'Any') -> 'DataFrame'>,\n",
       " 'read_feather': <function pandas.io.feather_format.read_feather(path: 'FilePath | ReadBuffer[bytes]', columns: 'Sequence[Hashable] | None' = None, use_threads: 'bool' = True, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame'>,\n",
       " 'read_gbq': <function pandas.io.gbq.read_gbq(query: 'str', project_id: 'str | None' = None, index_col: 'str | None' = None, col_order: 'list[str] | None' = None, reauth: 'bool' = False, auth_local_webserver: 'bool' = True, dialect: 'str | None' = None, location: 'str | None' = None, configuration: 'dict[str, Any] | None' = None, credentials: 'Credentials | None' = None, use_bqstorage_api: 'bool | None' = None, max_results: 'int | None' = None, progress_bar_type: 'str | None' = None) -> 'DataFrame'>,\n",
       " 'read_html': <function pandas.io.html.read_html(io: 'FilePath | ReadBuffer[str]', *, match: 'str | Pattern' = '.+', flavor: 'HTMLFlavors | Sequence[HTMLFlavors] | None' = None, header: 'int | Sequence[int] | None' = None, index_col: 'int | Sequence[int] | None' = None, skiprows: 'int | Sequence[int] | slice | None' = None, attrs: 'dict[str, str] | None' = None, parse_dates: 'bool' = False, thousands: 'str | None' = ',', encoding: 'str | None' = None, decimal: 'str' = '.', converters: 'dict | None' = None, na_values: 'Iterable[object] | None' = None, keep_default_na: 'bool' = True, displayed_only: 'bool' = True, extract_links: \"Literal[None, 'header', 'footer', 'body', 'all']\" = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, storage_options: 'StorageOptions' = None) -> 'list[DataFrame]'>,\n",
       " 'read_xml': <function pandas.io.xml.read_xml(path_or_buffer: 'FilePath | ReadBuffer[bytes] | ReadBuffer[str]', *, xpath: 'str' = './*', namespaces: 'dict[str, str] | None' = None, elems_only: 'bool' = False, attrs_only: 'bool' = False, names: 'Sequence[str] | None' = None, dtype: 'DtypeArg | None' = None, converters: 'ConvertersArg | None' = None, parse_dates: 'ParseDatesArg | None' = None, encoding: 'str | None' = 'utf-8', parser: 'XMLParsers' = 'lxml', stylesheet: 'FilePath | ReadBuffer[bytes] | ReadBuffer[str] | None' = None, iterparse: 'dict[str, list[str]] | None' = None, compression: 'CompressionOptions' = 'infer', storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame'>,\n",
       " 'read_json': <function pandas.io.json._json.read_json(path_or_buf: 'FilePath | ReadBuffer[str] | ReadBuffer[bytes]', *, orient: 'str | None' = None, typ: \"Literal['frame', 'series']\" = 'frame', dtype: 'DtypeArg | None' = None, convert_axes: 'bool | None' = None, convert_dates: 'bool | list[str]' = True, keep_default_dates: 'bool' = True, precise_float: 'bool' = False, date_unit: 'str | None' = None, encoding: 'str | None' = None, encoding_errors: 'str | None' = 'strict', lines: 'bool' = False, chunksize: 'int | None' = None, compression: 'CompressionOptions' = 'infer', nrows: 'int | None' = None, storage_options: 'StorageOptions | None' = None, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>, engine: 'JSONEngine' = 'ujson') -> 'DataFrame | Series | JsonReader'>,\n",
       " 'read_stata': <function pandas.io.stata.read_stata(filepath_or_buffer: 'FilePath | ReadBuffer[bytes]', *, convert_dates: 'bool' = True, convert_categoricals: 'bool' = True, index_col: 'str | None' = None, convert_missing: 'bool' = False, preserve_dtypes: 'bool' = True, columns: 'Sequence[str] | None' = None, order_categoricals: 'bool' = True, chunksize: 'int | None' = None, iterator: 'bool' = False, compression: 'CompressionOptions' = 'infer', storage_options: 'StorageOptions | None' = None) -> 'DataFrame | StataReader'>,\n",
       " 'read_sas': <function pandas.io.sas.sasreader.read_sas(filepath_or_buffer: 'FilePath | ReadBuffer[bytes]', *, format: 'str | None' = None, index: 'Hashable | None' = None, encoding: 'str | None' = None, chunksize: 'int | None' = None, iterator: 'bool' = False, compression: 'CompressionOptions' = 'infer') -> 'DataFrame | ReaderBase'>,\n",
       " 'read_spss': <function pandas.io.spss.read_spss(path: 'str | Path', usecols: 'Sequence[str] | None' = None, convert_categoricals: 'bool' = True, dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>) -> 'DataFrame'>,\n",
       " 'json_normalize': <function pandas.io.json._normalize.json_normalize(data: 'dict | list[dict]', record_path: 'str | list | None' = None, meta: 'str | list[str | list[str]] | None' = None, meta_prefix: 'str | None' = None, record_prefix: 'str | None' = None, errors: 'IgnoreRaise' = 'raise', sep: 'str' = '.', max_level: 'int | None' = None) -> 'DataFrame'>,\n",
       " 'test': <function pandas.util._tester.test(extra_args: 'list[str] | None' = None, run_doctests: 'bool' = False) -> 'None'>,\n",
       " '_built_with_meson': True,\n",
       " '_version_meson': <module 'pandas._version_meson' from 'c:\\\\Users\\\\cikeg\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\pandas\\\\_version_meson.py'>,\n",
       " '__version__': '2.2.3',\n",
       " '__git_version__': '0691c5cf90477d3503834d983f69350f250a6ff7',\n",
       " '__all__': ['ArrowDtype',\n",
       "  'BooleanDtype',\n",
       "  'Categorical',\n",
       "  'CategoricalDtype',\n",
       "  'CategoricalIndex',\n",
       "  'DataFrame',\n",
       "  'DateOffset',\n",
       "  'DatetimeIndex',\n",
       "  'DatetimeTZDtype',\n",
       "  'ExcelFile',\n",
       "  'ExcelWriter',\n",
       "  'Flags',\n",
       "  'Float32Dtype',\n",
       "  'Float64Dtype',\n",
       "  'Grouper',\n",
       "  'HDFStore',\n",
       "  'Index',\n",
       "  'IndexSlice',\n",
       "  'Int16Dtype',\n",
       "  'Int32Dtype',\n",
       "  'Int64Dtype',\n",
       "  'Int8Dtype',\n",
       "  'Interval',\n",
       "  'IntervalDtype',\n",
       "  'IntervalIndex',\n",
       "  'MultiIndex',\n",
       "  'NA',\n",
       "  'NaT',\n",
       "  'NamedAgg',\n",
       "  'Period',\n",
       "  'PeriodDtype',\n",
       "  'PeriodIndex',\n",
       "  'RangeIndex',\n",
       "  'Series',\n",
       "  'SparseDtype',\n",
       "  'StringDtype',\n",
       "  'Timedelta',\n",
       "  'TimedeltaIndex',\n",
       "  'Timestamp',\n",
       "  'UInt16Dtype',\n",
       "  'UInt32Dtype',\n",
       "  'UInt64Dtype',\n",
       "  'UInt8Dtype',\n",
       "  'api',\n",
       "  'array',\n",
       "  'arrays',\n",
       "  'bdate_range',\n",
       "  'concat',\n",
       "  'crosstab',\n",
       "  'cut',\n",
       "  'date_range',\n",
       "  'describe_option',\n",
       "  'errors',\n",
       "  'eval',\n",
       "  'factorize',\n",
       "  'get_dummies',\n",
       "  'from_dummies',\n",
       "  'get_option',\n",
       "  'infer_freq',\n",
       "  'interval_range',\n",
       "  'io',\n",
       "  'isna',\n",
       "  'isnull',\n",
       "  'json_normalize',\n",
       "  'lreshape',\n",
       "  'melt',\n",
       "  'merge',\n",
       "  'merge_asof',\n",
       "  'merge_ordered',\n",
       "  'notna',\n",
       "  'notnull',\n",
       "  'offsets',\n",
       "  'option_context',\n",
       "  'options',\n",
       "  'period_range',\n",
       "  'pivot',\n",
       "  'pivot_table',\n",
       "  'plotting',\n",
       "  'qcut',\n",
       "  'read_clipboard',\n",
       "  'read_csv',\n",
       "  'read_excel',\n",
       "  'read_feather',\n",
       "  'read_fwf',\n",
       "  'read_gbq',\n",
       "  'read_hdf',\n",
       "  'read_html',\n",
       "  'read_json',\n",
       "  'read_orc',\n",
       "  'read_parquet',\n",
       "  'read_pickle',\n",
       "  'read_sas',\n",
       "  'read_spss',\n",
       "  'read_sql',\n",
       "  'read_sql_query',\n",
       "  'read_sql_table',\n",
       "  'read_stata',\n",
       "  'read_table',\n",
       "  'read_xml',\n",
       "  'reset_option',\n",
       "  'set_eng_float_format',\n",
       "  'set_option',\n",
       "  'show_versions',\n",
       "  'test',\n",
       "  'testing',\n",
       "  'timedelta_range',\n",
       "  'to_datetime',\n",
       "  'to_numeric',\n",
       "  'to_pickle',\n",
       "  'to_timedelta',\n",
       "  'tseries',\n",
       "  'unique',\n",
       "  'value_counts',\n",
       "  'wide_to_long']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame basics\n",
    "\n",
    "### A few of the fundamental routines for selecting, sorting, adding and aggregating data in DataFrames\n",
    "\n",
    "Difficulty: *easy*\n",
    "\n",
    "Note: remember to import numpy using:\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "Consider the following Python dictionary `data` and Python list `labels`:\n",
    "\n",
    "``` python\n",
    "data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n",
    "        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n",
    "        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n",
    "        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}\n",
    "\n",
    "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "```\n",
    "(This is just some meaningless data I made up with the theme of animals and trips to a vet.)\n",
    "\n",
    "**4.** Create a DataFrame `df` from this dictionary `data` which has the index `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n",
    "        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n",
    "        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n",
    "        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}\n",
    "\n",
    "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "\n",
    "df = pd.Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Display a summary of the basic information about this DataFrame and its data (*hint: there is a single method that can be called on the DataFrame*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.** Return the first 3 rows of the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.** Select just the 'animal' and 'age' columns from the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.** Select the data in rows `[3, 4, 8]` *and* in columns `['animal', 'age']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.** Select only the rows where the number of visits is greater than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10.** Select the rows where the age is missing, i.e. it is `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11.** Select the rows where the animal is a cat *and* the age is less than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.** Select the rows the age is between 2 and 4 (inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13.** Change the age in row 'f' to 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14.** Calculate the sum of all visits in `df` (i.e. find the total number of visits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15.** Calculate the mean age for each different animal in `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16.** Append a new row 'k' to `df` with your choice of values for each column. Then delete that row to return the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17.** Count the number of each type of animal in `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18.** Sort `df` first by the values in the 'age' in *decending* order, then by the value in the 'visits' column in *ascending* order (so row `i` should be first, and row `d` should be last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**19.** The 'priority' column contains the values 'yes' and 'no'. Replace this column with a column of boolean values: 'yes' should be `True` and 'no' should be `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20.** In the 'animal' column, change the 'snake' entries to 'python'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**21.** For each animal type and each number of visits, find the mean age. In other words, each row is an animal, each column is a number of visits and the values are the mean ages (*hint: use a pivot table*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames: beyond the basics\n",
    "\n",
    "### Slightly trickier: you may need to combine two or more methods to get the right answer\n",
    "\n",
    "Difficulty: *medium*\n",
    "\n",
    "The previous section was tour through some basic but essential DataFrame operations. Below are some ways that you might need to cut your data, but for which there is no single \"out of the box\" method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**22.** You have a DataFrame `df` with a column 'A' of integers. For example:\n",
    "```python\n",
    "df = pd.DataFrame({'A': [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 7]})\n",
    "```\n",
    "\n",
    "How do you filter out rows which contain the same integer as the row immediately above?\n",
    "\n",
    "You should be left with a column containing the following values:\n",
    "\n",
    "```python\n",
    "1, 2, 3, 4, 5, 6, 7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**23.** Given a DataFrame of numeric values, say\n",
    "```python\n",
    "df = pd.DataFrame(np.random.random(size=(5, 3))) # a 5x3 frame of float values\n",
    "```\n",
    "\n",
    "how do you subtract the row mean from each element in the row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**24.** Suppose you have DataFrame with 10 columns of real numbers, for example:\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame(np.random.random(size=(5, 10)), columns=list('abcdefghij'))\n",
    "```\n",
    "Which column of numbers has the smallest sum?  Return that column's label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**25.** How do you count how many unique rows a DataFrame has (i.e. ignore all rows that are duplicates)? As input, use a DataFrame of zeros and ones with 10 rows and 3 columns.\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame(np.random.randint(0, 2, size=(10, 3)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three puzzles are slightly harder.\n",
    "\n",
    "\n",
    "**26.** In the cell below, you have a DataFrame `df` that consists of 10 columns of floating-point numbers. Exactly 5 entries in each row are NaN values. \n",
    "\n",
    "For each row of the DataFrame, find the *column* which contains the *third* NaN value.\n",
    "\n",
    "You should return a Series of column labels: `e, c, d, h, d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = np.nan\n",
    "\n",
    "data = [[0.04,  nan,  nan, 0.25,  nan, 0.43, 0.71, 0.51,  nan,  nan],\n",
    "        [ nan,  nan,  nan, 0.04, 0.76,  nan,  nan, 0.67, 0.76, 0.16],\n",
    "        [ nan,  nan, 0.5 ,  nan, 0.31, 0.4 ,  nan,  nan, 0.24, 0.01],\n",
    "        [0.49,  nan,  nan, 0.62, 0.73, 0.26, 0.85,  nan,  nan,  nan],\n",
    "        [ nan,  nan, 0.41,  nan, 0.05,  nan, 0.61,  nan, 0.48, 0.68]]\n",
    "\n",
    "columns = list('abcdefghij')\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# write a solution to the question here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**27.** A DataFrame has a column of groups 'grps' and and column of integer values 'vals': \n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({'grps': list('aaabbcaabcccbbc'), \n",
    "                   'vals': [12,345,3,1,45,14,4,52,54,23,235,21,57,3,87]})\n",
    "```\n",
    "For each *group*, find the sum of the three greatest values. You should end up with the answer as follows:\n",
    "```\n",
    "grps\n",
    "a    409\n",
    "b    156\n",
    "c    345\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'grps': list('aaabbcaabcccbbc'), \n",
    "                   'vals': [12,345,3,1,45,14,4,52,54,23,235,21,57,3,87]})\n",
    "\n",
    "# write a solution to the question here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**28.** The DataFrame `df` constructed below has two integer columns 'A' and 'B'. The values in 'A' are between 1 and 100 (inclusive). \n",
    "\n",
    "For each group of 10 consecutive integers in 'A' (i.e. `(0, 10]`, `(10, 20]`, ...), calculate the sum of the corresponding values in column 'B'.\n",
    "\n",
    "The answer should be a Series as follows:\n",
    "\n",
    "```\n",
    "A\n",
    "(0, 10]      635\n",
    "(10, 20]     360\n",
    "(20, 30]     315\n",
    "(30, 40]     306\n",
    "(40, 50]     750\n",
    "(50, 60]     284\n",
    "(60, 70]     424\n",
    "(70, 80]     526\n",
    "(80, 90]     835\n",
    "(90, 100]    852\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.RandomState(8765).randint(1, 101, size=(100, 2)), columns = [\"A\", \"B\"])\n",
    "\n",
    "# write a solution to the question here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames: harder problems \n",
    "\n",
    "### These might require a bit of thinking outside the box...\n",
    "\n",
    "...but all are solvable using just the usual pandas/NumPy methods (and so avoid using explicit `for` loops).\n",
    "\n",
    "Difficulty: *hard*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**29.** Consider a DataFrame `df` where there is an integer column 'X':\n",
    "```python\n",
    "df = pd.DataFrame({'X': [7, 2, 0, 3, 4, 2, 5, 0, 3, 4]})\n",
    "```\n",
    "For each value, count the difference back to the previous zero (or the start of the Series, whichever is closer). These values should therefore be \n",
    "\n",
    "```\n",
    "[1, 2, 0, 1, 2, 3, 4, 0, 1, 2]\n",
    "```\n",
    "\n",
    "Make this a new column 'Y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**30.** Consider the DataFrame constructed below which contains rows and columns of numerical data. \n",
    "\n",
    "Create a list of the column-row index locations of the 3 largest values in this DataFrame. In this case, the answer should be:\n",
    "```\n",
    "[(5, 7), (6, 4), (2, 5)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.RandomState(30).randint(1, 101, size=(8, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**31.** You are given the DataFrame below with a column of group IDs, 'grps', and a column of corresponding integer values, 'vals'.\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({\"vals\": np.random.RandomState(31).randint(-30, 30, size=15), \n",
    "                   \"grps\": np.random.RandomState(31).choice([\"A\", \"B\"], 15)})\n",
    "```\n",
    "\n",
    "Create a new column 'patched_values' which contains the same values as the 'vals' any negative values in 'vals' with the group mean:\n",
    "\n",
    "```\n",
    "    vals grps  patched_vals\n",
    "0    -12    A          13.6\n",
    "1     -7    B          28.0\n",
    "2    -14    A          13.6\n",
    "3      4    A           4.0\n",
    "4     -7    A          13.6\n",
    "5     28    B          28.0\n",
    "6     -2    A          13.6\n",
    "7     -1    A          13.6\n",
    "8      8    A           8.0\n",
    "9     -2    B          28.0\n",
    "10    28    A          28.0\n",
    "11    12    A          12.0\n",
    "12    16    A          16.0\n",
    "13   -24    A          13.6\n",
    "14   -12    A          13.6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**32.** Implement a rolling mean over groups with window size 3, which ignores NaN value. For example consider the following DataFrame:\n",
    "\n",
    "```python\n",
    ">>> df = pd.DataFrame({'group': list('aabbabbbabab'),\n",
    "                       'value': [1, 2, 3, np.nan, 2, 3, np.nan, 1, 7, 3, np.nan, 8]})\n",
    ">>> df\n",
    "   group  value\n",
    "0      a    1.0\n",
    "1      a    2.0\n",
    "2      b    3.0\n",
    "3      b    NaN\n",
    "4      a    2.0\n",
    "5      b    3.0\n",
    "6      b    NaN\n",
    "7      b    1.0\n",
    "8      a    7.0\n",
    "9      b    3.0\n",
    "10     a    NaN\n",
    "11     b    8.0\n",
    "```\n",
    "The goal is to compute the Series:\n",
    "\n",
    "```\n",
    "0     1.000000\n",
    "1     1.500000\n",
    "2     3.000000\n",
    "3     3.000000\n",
    "4     1.666667\n",
    "5     3.000000\n",
    "6     3.000000\n",
    "7     2.000000\n",
    "8     3.666667\n",
    "9     2.000000\n",
    "10    4.500000\n",
    "11    4.000000\n",
    "```\n",
    "E.g. the first window of size three for group 'b' has values 3.0, NaN and 3.0 and occurs at row index 5. Instead of being NaN the value in the new column at this row index should be 3.0 (just the two non-NaN values are used to compute the mean (3+3)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series and DatetimeIndex\n",
    "\n",
    "### Exercises for creating and manipulating Series with datetime data\n",
    "\n",
    "Difficulty: *easy/medium*\n",
    "\n",
    "pandas is fantastic for working with dates and times. These puzzles explore some of this functionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**33.** Create a DatetimeIndex that contains each business day of 2015 and use it to index a Series of random numbers. Let's call this Series `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**34.** Find the sum of the values in `s` for every Wednesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**35.** For each calendar month in `s`, find the mean of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**36.** For each group of four consecutive calendar months in `s`, find the date on which the highest value occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**37.** Create a DateTimeIndex consisting of the third Thursday in each month for the years 2015 and 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "### Making a DataFrame easier to work with\n",
    "\n",
    "Difficulty: *easy/medium*\n",
    "\n",
    "It happens all the time: someone gives you data containing malformed strings, Python, lists and missing data. How do you tidy it up so you can get on with the analysis?\n",
    "\n",
    "Take this monstrosity as the DataFrame to use in the following puzzles:\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm', \n",
    "                               'Budapest_PaRis', 'Brussels_londOn'],\n",
    "              'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],\n",
    "              'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],\n",
    "                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )', \n",
    "                               '12. Air France', '\"Swiss Air\"']})\n",
    "```\n",
    "Formatted, it looks like this:\n",
    "\n",
    "```\n",
    "            From_To  FlightNumber  RecentDelays              Airline\n",
    "0      LoNDon_paris       10045.0      [23, 47]               KLM(!)\n",
    "1      MAdrid_miLAN           NaN            []    <Air France> (12)\n",
    "2  londON_StockhOlm       10065.0  [24, 43, 87]  (British Airways. )\n",
    "3    Budapest_PaRis           NaN          [13]       12. Air France\n",
    "4   Brussels_londOn       10085.0      [67, 32]          \"Swiss Air\"\n",
    "```\n",
    "\n",
    "\n",
    "(It's some flight data I made up; it's not meant to be accurate in any way.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**38.** Some values in the the **FlightNumber** column are missing (they are `NaN`). These numbers are meant to increase by 10 with each row so 10055 and 10075 need to be put in place. Modify `df` to fill in these missing numbers and make the column an integer column (instead of a float column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**39.** The **From\\_To** column would be better as two separate columns! Split each string on the underscore delimiter `_` to give a new temporary DataFrame called 'temp' with the correct values. Assign the correct column names 'From' and 'To' to this temporary DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**40.** Notice how the capitalisation of the city names is all mixed up in this temporary DataFrame 'temp'. Standardise the strings so that only the first letter is uppercase (e.g. \"londON\" should become \"London\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**41.** Delete the **From_To** column from `df` and attach the temporary DataFrame 'temp' from the previous questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**42**. In the **Airline** column, you can see some extra puctuation and symbols have appeared around the airline names. Pull out just the airline name. E.g. `'(British Airways. )'` should become `'British Airways'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**43**. In the RecentDelays column, the values have been entered into the DataFrame as a list. We would like each first value in its own column, each second value in its own column, and so on. If there isn't an Nth value, the value should be NaN.\n",
    "\n",
    "Expand the Series of lists into a DataFrame named `delays`, rename the columns `delay_1`, `delay_2`, etc. and replace the unwanted RecentDelays column in `df` with `delays`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame should look much better now.\n",
    "```\n",
    "   FlightNumber          Airline      From         To  delay_1  delay_2  delay_3\n",
    "0         10045              KLM    London      Paris     23.0     47.0      NaN\n",
    "1         10055       Air France    Madrid      Milan      NaN      NaN      NaN\n",
    "2         10065  British Airways    London  Stockholm     24.0     43.0     87.0\n",
    "3         10075       Air France  Budapest      Paris     13.0      NaN      NaN\n",
    "4         10085        Swiss Air  Brussels     London     67.0     32.0      NaN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using MultiIndexes\n",
    "\n",
    "### Go beyond flat DataFrames with additional index levels\n",
    "\n",
    "Difficulty: *medium*\n",
    "\n",
    "Previous exercises have seen us analysing data from DataFrames equipped with a single index level. However, pandas also gives you the possibilty of indexing your data using *multiple* levels. This is very much like adding new dimensions to a Series or a DataFrame. For example, a Series is 1D, but by using a MultiIndex with 2 levels we gain of much the same functionality as a 2D DataFrame.\n",
    "\n",
    "The set of puzzles below explores how you might use multiple index levels to enhance data analysis.\n",
    "\n",
    "To warm up, we'll look make a Series with two index levels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**44**. Given the lists `letters = ['A', 'B', 'C']` and `numbers = list(range(10))`, construct a MultiIndex object from the product of the two lists. Use it to index a Series of random numbers. Call this Series `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**45.** Check the index of `s` is lexicographically sorted (this is a necessary proprty for indexing to work correctly with a MultiIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**46**. Select the labels `1`, `3` and `6` from the second level of the MultiIndexed Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**47**. Slice the Series `s`; slice up to label 'B' for the first level and from label 5 onwards for the second level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**48**. Sum the values in `s` for each label in the first level (you should have Series giving you a total for labels A, B and C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**49**. Suppose that `sum()` (and other methods) did not accept a `level` keyword argument. How else could you perform the equivalent of `s.sum(level=1)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**50**. Exchange the levels of the MultiIndex so we have an index of the form (letters, numbers). Is this new Series properly lexsorted? If not, sort it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minesweeper\n",
    "\n",
    "### Generate the numbers for safe squares in a Minesweeper grid\n",
    "\n",
    "Difficulty: *medium* to *hard*\n",
    "\n",
    "If you've ever used an older version of Windows, there's a good chance you've played with Minesweeper:\n",
    "- https://en.wikipedia.org/wiki/Minesweeper_(video_game)\n",
    "\n",
    "\n",
    "If you're not familiar with the game, imagine a grid of squares: some of these squares conceal a mine. If you click on a mine, you lose instantly. If you click on a safe square, you reveal a number telling you how many mines are found in the squares that are immediately adjacent. The aim of the game is to uncover all squares in the grid that do not contain a mine.\n",
    "\n",
    "In this section, we'll make a DataFrame that contains the necessary data for a game of Minesweeper: coordinates of the squares, whether the square contains a mine and the number of mines found on adjacent squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**51**. Let's suppose we're playing Minesweeper on a 5 by 4 grid, i.e.\n",
    "```\n",
    "X = 5\n",
    "Y = 4\n",
    "```\n",
    "To begin, generate a DataFrame `df` with two columns, `'x'` and `'y'` containing every coordinate for this grid. That is, the DataFrame should start:\n",
    "```\n",
    "   x  y\n",
    "0  0  0\n",
    "1  0  1\n",
    "2  0  2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**52**. For this DataFrame `df`, create a new column of zeros (safe) and ones (mine). The probability of a mine occuring at each location should be 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**53**. Now create a new column for this DataFrame called `'adjacent'`. This column should contain the number of mines found on adjacent squares in the grid. \n",
    "\n",
    "(E.g. for the first row, which is the entry for the coordinate `(0, 0)`, count how many mines are found on the coordinates `(0, 1)`, `(1, 0)` and `(1, 1)`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**54**. For rows of the DataFrame that contain a mine, set the value in the `'adjacent'` column to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**55**. Finally, convert the DataFrame to grid of the adjacent mine counts: columns are the `x` coordinate, rows are the `y` coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "### Visualize trends and patterns in data\n",
    "\n",
    "Difficulty: *medium*\n",
    "\n",
    "To really get a good understanding of the data contained in your DataFrame, it is often essential to create plots: if you're lucky, trends and anomalies will jump right out at you. This functionality is baked into pandas and the puzzles below explore some of what's possible with the library.\n",
    "\n",
    "**56.** Pandas is highly integrated with the plotting library matplotlib, and makes plotting DataFrames very user-friendly! Plotting in a notebook environment usually makes use of the following boilerplate:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "```\n",
    "\n",
    "matplotlib is the plotting library which pandas' plotting functionality is built upon, and it is usually aliased to ```plt```.\n",
    "\n",
    "```%matplotlib inline``` tells the notebook to show plots inline, instead of creating them in a separate window.  \n",
    "\n",
    "```plt.style.use('ggplot')``` is a style theme that most people find agreeable, based upon the styling of R's ggplot package.\n",
    "\n",
    "For starters, make a scatter plot of this random data, but use black X's instead of the default markers. \n",
    "\n",
    "```df = pd.DataFrame({\"xs\":[1,5,2,8,1], \"ys\":[4,2,1,9,6]})```\n",
    "\n",
    "Consult the [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) if you get stuck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**57.** Columns in your DataFrame can also be used to modify colors and sizes.  Bill has been keeping track of his performance at work over time, as well as how good he was feeling that day, and whether he had a cup of coffee in the morning.  Make a plot which incorporates all four features of this DataFrame.\n",
    "\n",
    "(Hint:  If you're having trouble seeing the plot, try multiplying the Series which you choose to represent size by 10 or more)\n",
    "\n",
    "*The chart doesn't have to be pretty: this isn't a course in data viz!*\n",
    "\n",
    "```\n",
    "df = pd.DataFrame({\"productivity\":[5,2,3,1,4,5,6,7,8,3,4,8,9],\n",
    "                   \"hours_in\"    :[1,9,6,5,3,9,2,9,1,7,4,2,2],\n",
    "                   \"happiness\"   :[2,1,3,2,3,1,2,3,1,2,2,1,3],\n",
    "                   \"caffienated\" :[0,0,1,1,0,0,0,0,1,1,0,1,0]})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**58.**  What if we want to plot multiple things?  Pandas allows you to pass in a matplotlib *Axis* object for plots, and plots will also return an Axis object.\n",
    "\n",
    "Make a bar plot of monthly revenue with a line plot of monthly advertising spending (numbers in millions)\n",
    "\n",
    "```\n",
    "df = pd.DataFrame({\"revenue\":[57,68,63,71,72,90,80,62,59,51,47,52],\n",
    "                   \"advertising\":[2.1,1.9,2.7,3.0,3.6,3.2,2.7,2.4,1.8,1.6,1.3,1.9],\n",
    "                   \"month\":range(12)\n",
    "                  })\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're finally ready to create a candlestick chart, which is a very common tool used to analyze stock price data.  A candlestick chart shows the opening, closing, highest, and lowest price for a stock during a time window.  The color of the \"candle\" (the thick part of the bar) is green if the stock closed above its opening price, or red if below.\n",
    "\n",
    "![Candlestick Example](img/candle.jpg)\n",
    "\n",
    "This was initially designed to be a pandas plotting challenge, but it just so happens that this type of plot is just not feasible using pandas' methods.  If you are unfamiliar with matplotlib, we have provided a function that will plot the chart for you so long as you can use pandas to get the data into the correct format.\n",
    "\n",
    "Your first step should be to get the data in the correct format using pandas' time-series grouping function.  We would like each candle to represent an hour's worth of data.  You can write your own aggregation function which returns the open/high/low/close, but pandas has a built-in which also does this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell contains helper functions.  Call ```day_stock_data()``` to generate a DataFrame containing the prices a hypothetical stock sold for, and the time the sale occurred.  Call ```plot_candlestick(df)``` on your properly aggregated and formatted stock data to print the candlestick chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def float_to_time(x):\n",
    "    return str(int(x)) + \":\" + str(int(x%1 * 60)).zfill(2) + \":\" + str(int(x*60 % 1 * 60)).zfill(2)\n",
    "\n",
    "def day_stock_data():\n",
    "    #NYSE is open from 9:30 to 4:00\n",
    "    time = 9.5\n",
    "    price = 100\n",
    "    results = [(float_to_time(time), price)]\n",
    "    while time < 16:\n",
    "        elapsed = np.random.exponential(.001)\n",
    "        time += elapsed\n",
    "        if time > 16:\n",
    "            break\n",
    "        price_diff = np.random.uniform(.999, 1.001)\n",
    "        price *= price_diff\n",
    "        results.append((float_to_time(time), price))\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(results, columns = ['time','price'])\n",
    "    df.time = pd.to_datetime(df.time)\n",
    "    return df\n",
    "\n",
    "#Don't read me unless you get stuck!\n",
    "def plot_candlestick(agg):\n",
    "    \"\"\"\n",
    "    agg is a DataFrame which has a DatetimeIndex and five columns: [\"open\",\"high\",\"low\",\"close\",\"color\"]\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    for time in agg.index:\n",
    "        ax.plot([time.hour] * 2, agg.loc[time, [\"high\",\"low\"]].values, color = \"black\")\n",
    "        ax.plot([time.hour] * 2, agg.loc[time, [\"open\",\"close\"]].values, color = agg.loc[time, \"color\"], linewidth = 10)\n",
    "\n",
    "    ax.set_xlim((8,16))\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.set_xlabel(\"Hour\")\n",
    "    ax.set_title(\"OHLC of Stock Value During Trading Day\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**59.** Generate a day's worth of random stock data, and aggregate / reformat it so that it has hourly summaries of the opening, highest, lowest, and closing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**60.** Now that you have your properly-formatted data, try to plot it yourself as a candlestick chart.  Use the ```plot_candlestick(df)``` function above, or matplotlib's [```plot``` documentation](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.plot.html) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*More exercises to follow soon...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
